{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across executions of the graph.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n",
    "# Note that this code does not actually perform any numeric operations; it\n",
    "# merely sets up the computational graph that we will later execute.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26902124.0\n",
      "21742188.0\n",
      "24783088.0\n",
      "33543312.0\n",
      "44482824.0\n",
      "47357692.0\n",
      "36176260.0\n",
      "17824000.0\n",
      "6596992.0\n",
      "2413751.8\n",
      "1177825.8\n",
      "774913.9\n",
      "599763.9\n",
      "494548.2\n",
      "417822.7\n",
      "356973.84\n",
      "307208.22\n",
      "265959.97\n",
      "231378.38\n",
      "202160.31\n",
      "177356.08\n",
      "156173.69\n",
      "137980.9\n",
      "122296.06\n",
      "108724.48\n",
      "96933.06\n",
      "86648.56\n",
      "77647.07\n",
      "69740.75\n",
      "62782.94\n",
      "56637.06\n",
      "51188.793\n",
      "46348.29\n",
      "42036.758\n",
      "38190.027\n",
      "34748.883\n",
      "31664.223\n",
      "28894.42\n",
      "26403.271\n",
      "24158.379\n",
      "22132.139\n",
      "20301.484\n",
      "18646.152\n",
      "17144.402\n",
      "15780.444\n",
      "14540.033\n",
      "13410.239\n",
      "12379.417\n",
      "11438.133\n",
      "10577.74\n",
      "9789.821\n",
      "9067.941\n",
      "8405.459\n",
      "7797.3564\n",
      "7238.9395\n",
      "6725.3613\n",
      "6252.3916\n",
      "5816.5063\n",
      "5414.3535\n",
      "5042.917\n",
      "4699.7354\n",
      "4382.29\n",
      "4088.8274\n",
      "3817.3396\n",
      "3565.5935\n",
      "3332.109\n",
      "3115.3838\n",
      "2914.1348\n",
      "2727.149\n",
      "2553.3406\n",
      "2391.5376\n",
      "2240.9373\n",
      "2100.6733\n",
      "1970.005\n",
      "1848.191\n",
      "1734.555\n",
      "1628.5248\n",
      "1529.4805\n",
      "1436.9451\n",
      "1350.4695\n",
      "1269.6274\n",
      "1193.9907\n",
      "1123.2041\n",
      "1056.9624\n",
      "994.91003\n",
      "936.7757\n",
      "882.29803\n",
      "831.20337\n",
      "783.35065\n",
      "738.47534\n",
      "696.3529\n",
      "656.807\n",
      "619.66095\n",
      "584.7593\n",
      "551.96704\n",
      "521.12695\n",
      "492.1281\n",
      "464.84668\n",
      "439.1785\n",
      "415.02502\n",
      "392.27548\n",
      "370.85562\n",
      "350.67352\n",
      "331.6603\n",
      "313.73737\n",
      "296.84576\n",
      "280.91693\n",
      "265.8938\n",
      "251.72003\n",
      "238.34561\n",
      "225.7199\n",
      "213.80058\n",
      "202.55258\n",
      "191.94446\n",
      "181.92416\n",
      "172.45378\n",
      "163.50418\n",
      "155.04425\n",
      "147.0487\n",
      "139.48354\n",
      "132.32727\n",
      "125.55955\n",
      "119.15376\n",
      "113.09126\n",
      "107.35513\n",
      "101.922005\n",
      "96.77945\n",
      "91.90645\n",
      "87.29264\n",
      "82.92043\n",
      "78.77849\n",
      "74.85408\n",
      "71.132645\n",
      "67.604996\n",
      "64.26016\n",
      "61.087154\n",
      "58.079826\n",
      "55.227463\n",
      "52.519363\n",
      "49.951378\n",
      "47.512573\n",
      "45.199055\n",
      "43.002964\n",
      "40.917885\n",
      "38.93881\n",
      "37.05784\n",
      "35.273247\n",
      "33.57676\n",
      "31.966547\n",
      "30.435957\n",
      "28.981165\n",
      "27.59827\n",
      "26.284218\n",
      "25.035496\n",
      "23.847736\n",
      "22.719372\n",
      "21.645845\n",
      "20.624775\n",
      "19.653576\n",
      "18.729586\n",
      "17.851173\n",
      "17.014914\n",
      "16.219814\n",
      "15.463282\n",
      "14.742764\n",
      "14.057087\n",
      "13.403677\n",
      "12.782437\n",
      "12.190584\n",
      "11.627541\n",
      "11.090944\n",
      "10.580059\n",
      "10.092948\n",
      "9.629826\n",
      "9.188345\n",
      "8.767515\n",
      "8.366483\n",
      "7.984625\n",
      "7.620383\n",
      "7.27357\n",
      "6.9432025\n",
      "6.628025\n",
      "6.327636\n",
      "6.041013\n",
      "5.7680707\n",
      "5.5075626\n",
      "5.2592525\n",
      "5.0226293\n",
      "4.7966375\n",
      "4.581513\n",
      "4.375779\n",
      "4.179846\n",
      "3.99257\n",
      "3.814342\n",
      "3.644139\n",
      "3.4815598\n",
      "3.326511\n",
      "3.1785827\n",
      "3.0373273\n",
      "2.9026465\n",
      "2.7739825\n",
      "2.6509833\n",
      "2.5336382\n",
      "2.4219525\n",
      "2.3149648\n",
      "2.2129695\n",
      "2.1155443\n",
      "2.0224872\n",
      "1.933686\n",
      "1.8486502\n",
      "1.7674487\n",
      "1.6901529\n",
      "1.6160958\n",
      "1.5453823\n",
      "1.4780353\n",
      "1.413384\n",
      "1.3518789\n",
      "1.2929661\n",
      "1.2366556\n",
      "1.1828183\n",
      "1.1314554\n",
      "1.0823288\n",
      "1.0353751\n",
      "0.99052095\n",
      "0.9476208\n",
      "0.9066151\n",
      "0.86736965\n",
      "0.82991225\n",
      "0.794061\n",
      "0.75981104\n",
      "0.7271246\n",
      "0.69581634\n",
      "0.66597533\n",
      "0.63736856\n",
      "0.60987955\n",
      "0.5837338\n",
      "0.55867887\n",
      "0.5347425\n",
      "0.51185524\n",
      "0.48988754\n",
      "0.46895856\n",
      "0.44897234\n",
      "0.4297811\n",
      "0.4114057\n",
      "0.3938226\n",
      "0.37704387\n",
      "0.36099908\n",
      "0.34563154\n",
      "0.33097422\n",
      "0.31688008\n",
      "0.30336028\n",
      "0.29046333\n",
      "0.27813086\n",
      "0.26635855\n",
      "0.25509477\n",
      "0.24428678\n",
      "0.23392397\n",
      "0.22407745\n",
      "0.21460842\n",
      "0.20551035\n",
      "0.1968188\n",
      "0.1885429\n",
      "0.1805778\n",
      "0.17293286\n",
      "0.1656549\n",
      "0.15870857\n",
      "0.15202329\n",
      "0.14559731\n",
      "0.1395235\n",
      "0.13363746\n",
      "0.12800917\n",
      "0.1226461\n",
      "0.11751616\n",
      "0.11256471\n",
      "0.107860036\n",
      "0.10330829\n",
      "0.09898445\n",
      "0.09486742\n",
      "0.0909085\n",
      "0.087101445\n",
      "0.08347736\n",
      "0.07996049\n",
      "0.07662684\n",
      "0.07341489\n",
      "0.070374355\n",
      "0.067463376\n",
      "0.064640015\n",
      "0.06197514\n",
      "0.059382334\n",
      "0.056884885\n",
      "0.054529164\n",
      "0.05226895\n",
      "0.05007321\n",
      "0.04801017\n",
      "0.04601392\n",
      "0.04411254\n",
      "0.04228773\n",
      "0.040533006\n",
      "0.038852643\n",
      "0.037242368\n",
      "0.03569421\n",
      "0.034211904\n",
      "0.03281751\n",
      "0.031451695\n",
      "0.030152382\n",
      "0.028913891\n",
      "0.027733939\n",
      "0.02658347\n",
      "0.025496308\n",
      "0.024450738\n",
      "0.023439474\n",
      "0.022489492\n",
      "0.021554008\n",
      "0.020677961\n",
      "0.019840885\n",
      "0.019033931\n",
      "0.01825348\n",
      "0.017522188\n",
      "0.016802538\n",
      "0.016110675\n",
      "0.015461728\n",
      "0.014834349\n",
      "0.014239768\n",
      "0.0136700105\n",
      "0.013119023\n",
      "0.012589259\n",
      "0.012085628\n",
      "0.011595362\n",
      "0.011132648\n",
      "0.010696164\n",
      "0.010258883\n",
      "0.009855779\n",
      "0.00946239\n",
      "0.009086583\n",
      "0.008730163\n",
      "0.008387966\n",
      "0.008051144\n",
      "0.00773157\n",
      "0.0074285036\n",
      "0.0071349517\n",
      "0.0068567554\n",
      "0.006594223\n",
      "0.0063395477\n",
      "0.006093595\n",
      "0.005855428\n",
      "0.0056282333\n",
      "0.0054152026\n",
      "0.005206025\n",
      "0.005004976\n",
      "0.004812875\n",
      "0.0046296837\n",
      "0.004451686\n",
      "0.0042867763\n",
      "0.004123687\n",
      "0.003973263\n",
      "0.0038202766\n",
      "0.0036784469\n",
      "0.003539215\n",
      "0.0034102215\n",
      "0.0032820606\n",
      "0.003165288\n",
      "0.0030494514\n",
      "0.002937938\n",
      "0.0028306092\n",
      "0.002729339\n",
      "0.0026283371\n",
      "0.002534189\n",
      "0.0024471097\n",
      "0.002361696\n",
      "0.0022759403\n",
      "0.0021984293\n",
      "0.0021227705\n",
      "0.0020507157\n",
      "0.0019779797\n",
      "0.0019111617\n",
      "0.0018458382\n",
      "0.0017810318\n",
      "0.0017230493\n",
      "0.001665595\n",
      "0.0016093294\n",
      "0.0015564675\n",
      "0.0015040676\n",
      "0.0014561558\n",
      "0.0014092703\n",
      "0.0013661933\n",
      "0.0013207281\n",
      "0.0012778422\n",
      "0.0012365105\n",
      "0.001197331\n",
      "0.0011606198\n",
      "0.001124331\n",
      "0.001086991\n",
      "0.0010531078\n",
      "0.0010212618\n",
      "0.000990041\n",
      "0.0009603192\n",
      "0.00093332794\n",
      "0.00090377085\n",
      "0.0008782229\n",
      "0.0008519518\n",
      "0.0008260942\n",
      "0.0008029299\n",
      "0.00077979953\n",
      "0.00075695966\n",
      "0.0007349797\n",
      "0.00071455596\n",
      "0.00069507293\n",
      "0.00067534053\n",
      "0.000656237\n",
      "0.0006385229\n",
      "0.000621186\n",
      "0.0006031827\n",
      "0.0005863132\n",
      "0.00057087746\n",
      "0.00055510574\n",
      "0.00054078497\n",
      "0.0005267089\n",
      "0.00051222526\n",
      "0.00049768266\n",
      "0.00048430974\n",
      "0.00047270305\n",
      "0.00046024044\n",
      "0.00044868328\n",
      "0.00043797327\n",
      "0.00042677266\n",
      "0.0004163013\n",
      "0.00040631212\n",
      "0.00039660814\n",
      "0.0003877772\n",
      "0.00037779444\n",
      "0.00036834183\n",
      "0.00035865195\n",
      "0.0003513606\n",
      "0.00034261198\n",
      "0.0003347133\n",
      "0.00032653913\n",
      "0.00031960983\n",
      "0.0003118984\n",
      "0.0003049036\n",
      "0.00029868205\n",
      "0.0002919094\n",
      "0.00028487478\n",
      "0.0002784228\n",
      "0.00027230632\n",
      "0.0002657894\n",
      "0.00026032113\n",
      "0.00025477592\n",
      "0.00024933522\n",
      "0.00024429802\n",
      "0.00023875407\n",
      "0.00023325924\n",
      "0.00022833163\n",
      "0.00022398497\n",
      "0.00021908757\n",
      "0.00021436003\n",
      "0.00021023705\n",
      "0.0002062993\n",
      "0.00020202427\n",
      "0.00019842727\n",
      "0.0001939699\n",
      "0.00018953142\n",
      "0.00018548829\n",
      "0.00018220434\n",
      "0.00017890686\n",
      "0.00017567091\n",
      "0.00017231447\n",
      "0.0001694735\n",
      "0.00016617958\n",
      "0.000163167\n",
      "0.00015992211\n",
      "0.00015724805\n",
      "0.0001540072\n",
      "0.00015125846\n",
      "0.00014846546\n",
      "0.00014558289\n",
      "0.00014314595\n",
      "0.00014090187\n",
      "0.00013866417\n",
      "0.0001360196\n",
      "0.0001338478\n",
      "0.00013137251\n",
      "0.00012901882\n",
      "0.0001271415\n",
      "0.00012521479\n",
      "0.00012317252\n",
      "0.00012170749\n",
      "0.00011922144\n",
      "0.00011738149\n",
      "0.00011552972\n",
      "0.000113425514\n",
      "0.00011185049\n",
      "0.00010975595\n",
      "0.00010760862\n",
      "0.00010619925\n",
      "0.000104483704\n",
      "0.000102996426\n",
      "0.00010113139\n",
      "9.97259e-05\n",
      "9.831794e-05\n",
      "9.656414e-05\n"
     ]
    }
   ],
   "source": [
    "# Now we have built our computational graph, so we enter a TensorFlow session to\n",
    "# actually execute the graph.\n",
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create numpy arrays holding the actual data for the inputs x and targets\n",
    "    # y\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for _ in range(500):\n",
    "        # Execute the graph many times. Each time it executes we want to bind\n",
    "        # x_value to x and y_value to y, specified with the feed_dict argument.\n",
    "        # Each time we execute the graph we want to compute the values for loss,\n",
    "        # new_w1, and new_w2; the values of these Tensors are returned as numpy\n",
    "        # arrays.\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                    feed_dict={x: x_value, y: y_value})\n",
    "        print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
